<!DOCTYPE html>
<html>
  <head>
  <meta charset=utf-8 />
  <title>  MCB111 Mathematics in Biology </title>
  <link rel="stylesheet" href="/Harvard-MCB111-2024-Fall/css/style.css" />
</head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<body>
  <div id="main">
 
    <header>
      <h3> MCB111: Mathematics in Biology (Fall 2024) </h3>
    </header>
 
    <nav role="navigation"> 
      <a href="/Harvard-MCB111-2024-Fall">Home</a>  | <a href="/Harvard-MCB111-2024-Fall/schedule.html">Schedule</a> | <a href="https://canvas.harvard.edu/courses/139673/">Canvas</a> | <a href="https://piazza.com/harvard/fall2024/mcb111">Piazza</a> | <a href="/Harvard-MCB111-2024-Fall/downloads/MCB111-syllabus.pdf">Syllabus [PDF]</a> | <a href="/Harvard-MCB111-2024-Fall/downloads/StudentHours_2024.pdf">Student hours schedule [PDF]</a> <br/>
      
      Lectures:
      <a href="/Harvard-MCB111-2024-Fall/w00/w00-lecture.html"> w00 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w01/w01-lecture.html"> w01 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-lecture.html"> w02 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w03/w03-lecture.html"> w03 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w04/w04-lecture.html"> w04 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w05/w05-lecture.html"> w05 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w06/w06-lecture.html"> w06 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w07/w07-lecture.html"> w07 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w08/w08-lecture.html"> w08 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w09/w09-lecture.html"> w09 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w10/w10-lecture.html"> w10 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w11/w11-lecture.html"> w11 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w12/w12-lecture.html"> w12 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w13/w13-lecture.html"> w13 </a> |
      <br/>
      
      inclass-notes:
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-notes-lecture1.pdf"> w02-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-notes-lecture2.pdf"> w02-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w03/w03-notes-lecture1.pdf"> w03-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w04/w04-notes-l1.pdf"> w04-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w05/w05-notes-lectures.pdf"> w05 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w06/w06-notes-l1.pdf"> w06-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w06/w06-notes-l2.pdf"> w06-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w07/w07-notes-l1.pdf"> w07-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w07/w07-notes-l2.pdf"> w07-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w08/w08-notes-l1.pdf"> w08-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w08/w08-notes-l2.pdf"> w08-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w09/w09-notes-l1.pdf"> w09-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w09/w09-notes-l2.pdf"> w09-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w10/w10-notes-l1.pdf"> w10-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w10/w10-notes-l2.pdf"> w10-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w10/w10-notes-l3.pdf"> w10-l3 </a> |
       <br/>
      
      inclass-code: 
      <a href="/Harvard-MCB111-2024-Fall/w00/w00-inclass.html"> w00 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w01/w01-sections_er_2024_complete.html"> w01 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-lecture1.html"> w02-l1 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-lecture2.html"> w02-l2 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w03/w03-lecture-code.ipynb"> w03 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w04/w04-lecture-code.ipynb"> w04 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w13/w13-code.ipynb"> w13 </a> |
      <br/>
      
      Sections: 
      <a href="/Harvard-MCB111-2024-Fall/w01/w01-sections.html"> w01 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-sections.html"> w02 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w03/w03-sections_NH_2024.ipynb"> w03 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w04/w04-sections_2024.ipynb"> w04 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w05/w05-sections_2024.ipynb"> w05 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w08/w08-sections_2024.ipynb"> w08 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w09/w09-sections_2024-ER.ipynb"> w09 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w10/w10-sections_2024-NH.ipynb"> w10 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w11/w11-sections_2024_er.ipynb"> w11 </a> |
     <br/>
      
      
      Homeworks:
      <a href="/Harvard-MCB111-2024-Fall/w00/w00-homework.html"> w00 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w01/w01-homework.html"> w01 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w02/w02-homework.html"> w02 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w03/w03-homework.html"> w03 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w04/w04-homework.html"> w04 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w05/w05-homework.html"> w05 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w06/w06-homework.html"> w06 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w07/w07-homework.html"> w07 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w08/w08-homework.html"> w08 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w09/w09-homework.html"> w09 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w10/w10-homework.html"> w10 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w11/w11-homework.html"> w11 </a> |
      <a href="/Harvard-MCB111-2024-Fall/w13/w13-homework.html"> w13 </a> |
     <br/>
      
      Hints: 
      <a href="/Harvard-MCB111-2024-Fall/w00/w00-hints.html"> w00 </a> |
      <br/>
      

      Final:
      <br/>
 
     </nav>
    
    <br/>
 
    <ul id="markdown-toc">
  <li><a href="#the-entropy-of-a-continuous-distribution-can-be-positive-or-negative" id="markdown-toc-the-entropy-of-a-continuous-distribution-can-be-positive-or-negative">The entropy of a continuous distribution can be positive or negative</a>    <ul>
      <li><a href="#the-shannon-entropy-of-the-exponential-distribution-can-be-negative" id="markdown-toc-the-shannon-entropy-of-the-exponential-distribution-can-be-negative">The Shannon entropy of the exponential distribution can be negative</a></li>
    </ul>
  </li>
</ul>

<h1 class="no_toc" id="week-01">week 01:</h1>

<h2 id="the-entropy-of-a-continuous-distribution-can-be-positive-or-negative">The entropy of a continuous distribution can be positive or negative</h2>

<p>In class, we introduced the entropy of a distribution</p>

<p>\begin{equation}
  \mbox{Entropy}(X) = H(X) = \sum_i p_i \log \frac{1}{p_i}.
\end{equation}</p>

<p>I mentioned that the entropy of a distribution was always positive, and could only be zero if one of the
elements \(i\) had probability \(p_i=1\).</p>

<p>I was following
<a href="http://worrydream.com/refs/Shannon%20-%20A%20Mathematical%20Theory%20of%20Communication.pdf">Shannon</a> (bottom of page 11),
but I failed to realize that Shannonâ€™s statement applies only to 
discrete distributions.</p>

<h3 id="the-shannon-entropy-of-the-exponential-distribution-can-be-negative">The Shannon entropy of the exponential distribution can be negative</h3>

<p>In fact, when you did your homework, you should have noticed, that, for the exponential distribution, given by the
probability density function</p>

<p>\begin{equation}
p(x) = \frac{1}{\mu}\exp^{-\frac{x}{\mu}}
\end{equation}</p>

<p>its entropy is given by</p>

<p>\begin{equation}
1 + \log(\mu),
\end{equation}</p>

<p>and it can take negative values for many values of the mean \(\mu\)!.</p>

<p>Robert B. Ash in his 1965 paper <a href="Information_Theory-Robert_Ash.pdf">Information
Theory</a> (page 237) noted this:
unlike a discrete distribution, for a continuous distribution, the
entropy can be positive or negative, in fact it may even be
\(+\infty\) or \(-\infty\).</p>

<p>The reason is that for a discrete distribution, the normalization condition</p>

<p>\begin{equation}
\sum_i p_i = 1
\end{equation}</p>

<p>requires that no individual probability can be larger than one, that
is, \(0\leq p_i \leq 1\) for all possible values of \(i\).</p>

<p>In contrast, for a continuous distribution, the normalization condition</p>

<p>\begin{equation}
\int p(x)\, dx = 1
\end{equation}</p>

<p>does not require for all probability density values, \(p(x)\), to be
smaller than one. For instance if you look at the wikipedia definition
of the <a href="https://en.wikipedia.org/wiki/Exponential_distribution">exponential
distribution</a>
you can see for \(\lambda = 1.5\) that some values of the probability
density function are indeed larger than one.</p>

<p>If we discretize the continuous distribution</p>

<p>\begin{equation}
\int p(x)\, dx \sim \sum_i p(x_i)\, \Delta = 1
\end{equation}</p>

<p>we see that only the product \(p(x_i) \Delta\) for each term \(i\) in the
sum has to be smaller than 1.</p>

<p>Having values of \(p(x)\) that can be larger than one is the cause why
the entropy of a continuous distribution</p>

<p>\begin{equation}
  H = \int  p(x)\, \log \frac{1}{p(x)}\, dx.
\end{equation}</p>

<p>cannot be guarantee to be positive, as the term \(\log \frac{1}{p(x)}\) takes
a negative value if \(p(x)&gt; 1\).</p>

<p>That means that for a continuous distribution the meaningful quantity to consider
is the relative entropy (or Kullback-Leibler distance) relative to some arbitrary
reference distribution \(p_0\). For any continuous distribution \(p(x)\) the
relative entropy</p>

<p>\begin{equation}
  D_{KL}(p||p_0) = \int  p(x)\, \log \frac{p(x)}{p_0(x)}\, dx,
\end{equation}</p>

<p>is guaranteed to be always positive.</p>

 
    <footer>
      <hr>
      <p>
        <a href="http://rivaslab.org">Elena Rivas</a> | Harvard University
    </footer>
 
  </div>
</body>
</html>
