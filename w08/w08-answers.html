<!DOCTYPE html>
<html>
  <head>
  <meta charset=utf-8 />
  <title>  MCB111 Mathematics in Biology </title>
  <link rel="stylesheet" href="/css/style.css" />
</head>

<script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script>

<body>
  <div id="main">
 
    <header>
      <h3> MCB111: Mathematics in Biology (Fall 2024) </h3>
    </header>
 
    <nav role="navigation"> 
      <a href="/Harvard-MCB111-2024-Fall">Home</a>  | <a href="/Harvard-MCB111-2024-Fall/schedule.html">Schedule</a> | <a href="https://canvas.harvard.edu/courses/139673/">Canvas</a> | <a href="https://piazza.com/harvard/fall2024/mcb111">Piazza</a> | <a href="/Harvard-MCB111-2024-Fall/downloads/MCB111-syllabus.pdf">Syllabus [PDF]</a> | <a href="/Harvard-MCB111-2024-Fall/downloads/StudentHours_2024.pdf">Student hours schedule [PDF]</a> <br/>
      
      Lectures:
      <a href="/w00/w00-lecture.html"> w00 </a> |
      <a href="/w01/w01-lecture.html"> w01 </a> |
      <a href="/w02/w02-lecture.html"> w02 </a> |
      <a href="/w03/w03-lecture.html"> w03 </a> |
      <a href="/w04/w04-lecture.html"> w04 </a> |
      <a href="/w05/w05-lecture.html"> w05 </a> |
      <a href="/w06/w06-lecture.html"> w06 </a> |
      <a href="/w07/w07-lecture.html"> w07 </a> |
      <a href="/w08/w08-lecture.html"> w08 </a> |
      <a href="/w09/w09-lecture.html"> w09 </a> |
      <a href="/w10/w10-lecture.html"> w10 </a> |
      <a href="/w11/w11-lecture.html"> w11 </a> |
      <a href="/w12/w12-lecture.html"> w12 </a> |
      <a href="/w13/w13-lecture.html"> w13 </a> |
      <br/>
      
      inclass-notes:
      <a href="/w02/w02-notes-lecture1.pdf"> w02-l1 </a> |
      <a href="/w02/w02-notes-lecture2.pdf"> w02-l2 </a> |
      <a href="/w03/w03-notes-lecture1.pdf"> w03-l1 </a> |
      <a href="/w04/w04-notes-l1.pdf"> w04-l1 </a> |
      <a href="/w05/w05-notes-lectures.pdf"> w05 </a> |
      <a href="/w06/w06-notes-l1.pdf"> w06-l1 </a> |
      <a href="/w06/w06-notes-l2.pdf"> w06-l2 </a> |
      <a href="/w07/w07-notes-l1.pdf"> w07-l1 </a> |
      <a href="/w07/w07-notes-l2.pdf"> w07-l2 </a> |
      <a href="/w08/w08-notes-l1.pdf"> w08-l1 </a> |
      <a href="/w08/w08-notes-l2.pdf"> w08-l2 </a> |
      <a href="/w09/w09-notes-l1.pdf"> w09-l1 </a> |
      <a href="/w09/w09-notes-l2.pdf"> w09-l2 </a> |
      <a href="/w10/w10-notes-l1.pdf"> w10-l1 </a> |
      <a href="/w10/w10-notes-l2.pdf"> w10-l2 </a> |
      <a href="/w10/w10-notes-l3.pdf"> w10-l3 </a> |
       <br/>
      
      inclass-code: 
      <a href="/w00/w00-inclass.html"> w00 </a> |
      <a href="/w01/w01-sections_er_2024_complete.html"> w01 </a> |
      <a href="/w02/w02-lecture1.html"> w02-l1 </a> |
      <a href="/w02/w02-lecture2.html"> w02-l2 </a> |
      <a href="/w03/w03-lecture-code.ipynb"> w03 </a> |
      <a href="/w04/w04-lecture-code.ipynb"> w04 </a> |
      <a href="/w13/w13-code.ipynb"> w13 </a> |
      <br/>
      
      Sections: 
      <a href="/w01/w01-sections.html"> w01 </a> |
      <a href="/w02/w02-sections.html"> w02 </a> |
      <a href="/w03/w03-sections_NH_2024.ipynb"> w03 </a> |
      <a href="/w04/w04-sections_2024.ipynb"> w04 </a> |
      <a href="/w05/w05-sections_2024.ipynb"> w05 </a> |
      <a href="/w08/w08-sections_2024.ipynb"> w08 </a> |
      <a href="/w09/w09-sections_2024-ER.ipynb"> w09 </a> |
      <a href="/w10/w10-sections_2024-NH.ipynb"> w10 </a> |
      <a href="/w11/w11-sections_2024_er.ipynb"> w11 </a> |
     <br/>
      
      
      Homeworks:
      <a href="/w00/w00-homework.html"> w00 </a> |
      <a href="/w01/w01-homework.html"> w01 </a> |
      <a href="/w02/w02-homework.html"> w02 </a> |
      <a href="/w03/w03-homework.html"> w03 </a> |
      <a href="/w04/w04-homework.html"> w04 </a> |
      <a href="/w05/w05-homework.html"> w05 </a> |
      <a href="/w06/w06-homework.html"> w06 </a> |
      <a href="/w07/w07-homework.html"> w07 </a> |
      <a href="/w08/w08-homework.html"> w08 </a> |
      <a href="/w09/w09-homework.html"> w09 </a> |
      <a href="/w10/w10-homework.html"> w10 </a> |
      <a href="/w11/w11-homework.html"> w11 </a> |
      <a href="/w13/w13-homework.html"> w13 </a> |
     <br/>
      
      Hints: 
      <a href="/w00/w00-hints.html"> w00 </a> |
      <br/>
      

      Final:
      <br/>
 
     </nav>
    
    <br/>
 
    <ul id="markdown-toc">
  <li><a href="#question-1-feed-forward-one-neuron-network" id="markdown-toc-question-1-feed-forward-one-neuron-network">Question 1: feed-forward one-neuron network</a>    <ul>
      <li><a href="#11-how-to-add-bias-to-the-model" id="markdown-toc-11-how-to-add-bias-to-the-model">1.1 how to add bias to the model</a></li>
      <li><a href="#12-steps-for-training" id="markdown-toc-12-steps-for-training">1.2 steps for training</a></li>
      <li><a href="#13-how-to-test-your-training-performance" id="markdown-toc-13-how-to-test-your-training-performance">1.3 how to test your training performance</a></li>
      <li><a href="#14-gradient-descent-and-stochastic-gradient-descent" id="markdown-toc-14-gradient-descent-and-stochastic-gradient-descent">1.4 gradient descent and stochastic gradient descent</a></li>
      <li><a href="#15-results" id="markdown-toc-15-results">1.5 Results</a></li>
    </ul>
  </li>
  <li><a href="#question-2-feedback-hopfield-network" id="markdown-toc-question-2-feedback-hopfield-network">Question 2: Feedback Hopfield network</a>    <ul>
      <li><a href="#21-weight-matrices" id="markdown-toc-21-weight-matrices">2.1 weight matrices:</a></li>
    </ul>
  </li>
</ul>

<h1 class="no_toc" id="week-08">week 08:</h1>

<p>(This homework has <strong>20p</strong> in total)</p>

<p>Code for this homework is available <a href="Answer_w08.m">Here</a></p>

<h2 id="question-1-feed-forward-one-neuron-network">Question 1: feed-forward one-neuron network</h2>
<p>The feed forward one-neuron network has been covered in the lecture week 08. There are a few points that you may take notice:</p>

<h3 id="11-how-to-add-bias-to-the-model">1.1 how to add bias to the model</h3>
<p>The original input for one-neuron network is:</p>

\[a = w_0 + \sum_{i=1}^I w_i x_i\]

<p>However, in the real implementation, the bias \(w_0\) doesn’t really fit in the scheme. To combine \(w_0\) into vector \(\mathbf{w}\), we can add a feature as \(x_0\) for all data (in here, each vector representing a letter)</p>

\[x_0 = 1\]

<p>Then the input can be converted into:</p>

\[a = \sum_{i=0}^I w_i x_i\]

<h3 id="12-steps-for-training">1.2 steps for training</h3>
<ol>
  <li>read data (please have a look of “loading data” section in attached <a href="ans_w08_readdata.m">code</a>)</li>
  <li>initialize w with small numbers</li>
  <li>set the weight for R(w) (named as alpha, which is a fixed parameter in loss function <a href="ans_w08_loss.m">code for loss function</a>)</li>
  <li>learning rate: this rate depends on how far each step moves in gradient descent algorithm. For stability, very small (~0.001) should be used.</li>
</ol>

<h3 id="13-how-to-test-your-training-performance">1.3 how to test your training performance</h3>
<p>After many iterations of training, the classification for training set could be really good.However, this is possibly due to overfitting.</p>

<p>To overcome this problem, I split the original training set into two subsets: first 90% of samples are used as training set, and the rest 10% are used as validation set. The model is trained by training set, and the prediction precision that reported in each round of training is calculated based on validation set.</p>

<h3 id="14-gradient-descent-and-stochastic-gradient-descent">1.4 gradient descent and stochastic gradient descent</h3>
<p><a href="ans_w08_SGD.m">function for SGD</a>
<a href="ans_w08_numericalgradient.m">function for numerical gradient</a>
I applied both algorithm in the provided code. As you may notice, both algorithm will allow convergence to almost the same prediction precision rate. Gradient descent has higher training speed, but SGD has higher robustness.</p>

<h3 id="15-results">1.5 Results</h3>

<div id="figcontainer">
   <div id="figure">
     <img src="loss_func.png" style="width:400px;" /> <br />
     Figure 1. loss function (cross entropy + R(w)) is decreasing during training.
   </div>
 </div>
<div id="figcontainer">
   <div id="figure">
     <img src="pred_rate.png" style="width:400px;" /> <br />
     Figure 2. Prediction rate is increasing during training and it will reach a plateau of ~93.5%
   </div>
 </div>

<h2 id="question-2-feedback-hopfield-network">Question 2: Feedback Hopfield network</h2>

<h3 id="21-weight-matrices">2.1 weight matrices:</h3>
<p>Construct Hopfield network based on lecture note <a href="ans_w08_createhopfield.m">Code is here</a></p>

<p>*Average error rate for c, d, e, x, z:</p>

<div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>    model     error c   error d   error e   error x   error z

    c         0.9860         0         0         0         0
    cd        0.8980    0.8635         0         0         0
    cde       0.6945    0.6400    0.3100         0         0
    cdex      0.3660    0.4185    0.1390    0.1900         0
    cdexz     0.2015    0.3155    0.0740    0.4385    0.3955 Therefore, after adding more letters into the model, the correct prediction rate get lower
</code></pre></div></div>

 
    <footer>
      <hr>
      <p>
        <a href="http://rivaslab.org">Elena Rivas</a> | Harvard University
    </footer>
 
  </div>
</body>
</html>
